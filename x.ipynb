{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Estimate Elasticity"}, {"metadata": {}, "cell_type": "markdown", "source": "In this use case, we will examine price elasticity for a regional retailer with 219 locations. The type of retailer isn't important. It could be a convenience store, a restaurant or coffee shop. Whatever the business of the retailer, their data should look basically the same.\n\nThe success of a retailer depends on several factors. One, is management and management decisions. Pricing, for example, is 100% controllable by management. Environmental factors surrounding the store are also very important and typically not very controllable. For example, a retailer will typically do better if the people living around the store make a bunch of money.\n\nOur data set in this exercise is a combination of controllable and non-controllable factors. The controllable factor is price. We also have numerous non-controllable environmental factors.\n\nOur goal in this exercise is to better understand the relationship between price, quantity and revenue. We achieve this goal by estimating the price elasticity of demand and the price elasticity of revenue."}, {"metadata": {}, "cell_type": "markdown", "source": "# Import data and data transformations"}, {"metadata": {}, "cell_type": "markdown", "source": "First, let's bring in the python libraries we will need in this exercise."}, {"metadata": {}, "cell_type": "code", "source": "!pip install chart_studio  --upgrade\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nimport numpy.dual as dual\nimport plotly as plotly\nimport sys\nimport types\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The next step is to upload the pricing data into object storage. To do this, click on the data icon (it is the one with two 1's and two 0's) in the northeast corner of the Watson Studio interface."}, {"metadata": {}, "cell_type": "markdown", "source": "Next, on the \"Files\" tab click on \"Browse\" where it says, \"Drop your file here or browse your files to add a new file\".\n\nFrom there, navigate to our data set \"RETAIL_DATA.csv\" and upload it to the cloud.\n\nOnce the file is uploaded it should appear in the data tab.\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "Once the file is loaded into object storage, \u00a0from the drop-down menu, select insert credentials.\n\nAfter this is complete, you should a cell should be created that looks like this.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "The next snippet of code will convert the file you just uploaded to object storage into a pandas data frame. Replace the credentials below with the credentials you inserted in the cell above."}, {"metadata": {}, "cell_type": "code", "source": "!rm RETAIL_DATA.csv\n!wget https://raw.githubusercontent.com/shadgriffin/Pricing_Tutorial/master/RETAIL_DATA.csv\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pd_datax = pd.read_csv(\"RETAIL_DATA.csv\")\n\ndf_retail = pd_datax\ndf_retail.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The first few rows of the data set should appear above.\n\nHere is a definition of each field.\n\nSTORE_ID\u200a-\u200ais a unique id specific to each retail outlet\n\nPERCENTAGE_OF_RENTERS is the percentage of households surrounding the store that rent their housing.\n\nPERCENTAGE_OF_CHILDREN is the percentage of households surrounding the store that have children.\n\nAVERAGE_INCOME is the average annual income of the households surrounding the store.\n\nAVERAGE_AGE_IN_YEARS is the average age of the head of household in the vicinity of the retail outlet.\n\nAVERAGE_LENGTH_OF_RESIDENCE is an average of the time individuals surrounding the retail outlet have lived at their current address.\n\nPERCENT_SPEAKING_SPANISH is the percentage of households surrounding the store that speak Spanish\n\nPRICE is the average price across multiple items sold at the retail outlet.\n\nQUANTITY is the number of items sold by the retail outlet in the last year.\n\nREVENUE is the total revenue for the store in the last year.\n\nOur goal in this exercise is to calculate the price elasticity of demand and the price elasticity of revenue. (Please look at the first notebook in this series to understand these terms.) This actually isn't that hard. To estimate an elasticity, you can use a standard ordinary least squares regression and natural log (base e) transformed variables.\n\nThe first step is to take the natural log each variable. The cell below does this.  Note the new variables created at the end of the DF."}, {"metadata": {}, "cell_type": "code", "source": "df_retail['LN_PRICE'] = np.log((df_retail.PRICE))\ndf_retail['LN_REVENUE'] = np.log((df_retail.REVENUE))\ndf_retail['LN_QUANTITY'] = np.log((df_retail.QUANTITY))\ndf_retail['LN_INCOME'] = np.log((df_retail.AVERAGE_INCOME))\ndf_retail['LN_AVERAGE_AGE_IN_YEARS'] = np.log((df_retail.AVERAGE_AGE_IN_YEARS))\ndf_retail['LN_AVERAGE_LENGTH_OF_RESIDENCE'] = np.log((df_retail.AVERAGE_LENGTH_OF_RESIDENCE))\ndf_retail['LN_PERCENT_SPEAKING_SPANISH'] = np.log((df_retail.PERCENT_SPEAKING_SPANISH))\ndf_retail['LN_PERCENT_HAVING_CHILDREN'] = np.log((df_retail.PERCENT_HAVING_CHILDREN))\ndf_retail['LN_PERCENTAGE_OF RENTERS'] = np.log((df_retail.PERCENTAGE_OF_RENTERS))\n\ndf_retail.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Build the Model"}, {"metadata": {}, "cell_type": "markdown", "source": "Now, let's build an ordinary least squares regression using the log transformed variables.\n\nDefine your independent and dependent variables with the following code cell. \n\nNote that ONLY variables that are statistically significant are included."}, {"metadata": {}, "cell_type": "code", "source": "independentx = df_retail[['LN_PRICE','LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n                          'LN_PERCENT_SPEAKING_SPANISH']]\nindependent = sm.add_constant(independentx, prepend=False)\ndependent=df_retail['LN_QUANTITY']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The next few cells run the OLS regression"}, {"metadata": {}, "cell_type": "code", "source": "mod = sm.OLS(dependent, independent)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "res = mod.fit()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(res.summary())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Interpreting the OLS Regression Coefficients"}, {"metadata": {}, "cell_type": "markdown", "source": "\nSo, the price elasticity of demand is -.64. This comes from the ANOVA table above and is the estimated coefficient of LN_PRICE regressed on LN_QUANTITY. \n\nThis means that a 1% increase in price will lower quantity sold by\u00a0.64%. \n\nThe other coefficients can be interpreted in a similar manner. \n\nA 1% increase in the average income of people around a store will increase the quantity sold by\u00a0.55%.\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "# Creating a Demand Curve"}, {"metadata": {}, "cell_type": "markdown", "source": "Now, let's make a demand curve. To do this, we will need to build a demand schedule using the predicted quantity at various prices based on our OLS regression model.\n\nFirst, let's summarize the variables other than price that are significant in the model. We will take the average and then evaluate the relationship between price and quantity when the other variables are at their mean."}, {"metadata": {}, "cell_type": "code", "source": "df_retail['chachacha']=1\nwookie = df_retail.groupby(['chachacha'])['PERCENTAGE_OF_RENTERS', 'PERCENT_HAVING_CHILDREN','AVERAGE_INCOME','PERCENT_SPEAKING_SPANISH'].mean()\n\n#wookie.reset_index(level=0, inplace=True)\nwookie.head()\n\n\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Next, we will create an array of prices and then convert the list to a pandas data frame."}, {"metadata": {}, "cell_type": "code", "source": "#create an array of prices\nprice = [1.50,1.75,2.0,2.25,2.50,2.75,3.0,3.25,3.50,3.75,4.0,4.25,4.50,4.75,5.0,5.25,5.50,5.75,6.0,6.25,6.50,6.75,\n         7.0,7.25,7.5,7.75,8.0,8.25,8.50,8.75,9.0,9.25,9.50,9.75,10.0,10.25,10.50,10.75,11.0,11.25,11.50,11.75,12.0]\ndf_price=pd.DataFrame(price)\ndf_price.columns = ['PRICE']\ndf_price['chachacha']=1\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Then, merge the average value for the non-price variables to the prices we constructed above and calculate the log of our variables."}, {"metadata": {}, "cell_type": "code", "source": "#join the array of prices to the average values of the other independent variables.\ndf_price =df_price.merge(wookie, on=['chachacha'], how='inner')\n\n#Create Log Transformed Variables\ndf_price['LN_PRICE'] = np.log((df_price.PRICE))\ndf_price['LN_INCOME'] = np.log((df_price.AVERAGE_INCOME))\ndf_price['LN_PERCENT_SPEAKING_SPANISH'] = np.log((df_price.PERCENT_SPEAKING_SPANISH))\ndf_price['LN_PERCENT_HAVING_CHILDREN'] = np.log((df_price.PERCENT_HAVING_CHILDREN))\ndf_price['LN_PERCENTAGE_OF RENTERS'] = np.log((df_price.PERCENTAGE_OF_RENTERS))\ndf_price['const']=1\n\ndf_price.head()\n\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now, we can use the input variables we manufactured in the previous few lines of code and our model to predict quantity at each price point. The end result will be a demand schedule."}, {"metadata": {}, "cell_type": "code", "source": "#create our scoring data set.\nscoring= df_price[['LN_PRICE','LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n                          'LN_PERCENT_SPEAKING_SPANISH','const']]\n#score the scoring data set\nln_q_hat=pd.DataFrame(res.predict(scoring))\n#name the columns correctly\nln_q_hat.columns = ['LN_Q_HAT']\n#combine ln of price and ln of predicted q into a new data frame\ndf_ce_demand = pd.concat([scoring['LN_PRICE'], ln_q_hat], axis=1)\n\n\n#exponentiate the ln variables to get predicted quantity and price\ndf_ce_demand['Q_HAT']=np.exp(df_ce_demand['LN_Q_HAT'])\ndf_ce_demand['PRICE']=np.exp(df_ce_demand['LN_PRICE'])\n\n#eliminate the ln variables and make the demand schedule.\ndf_ce_demand = df_ce_demand[['Q_HAT','PRICE']]\n\ndf_ce_demand.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Bingo! Now we have a demand schedule and we can plot it as a demand curve.\n\n"}, {"metadata": {}, "cell_type": "code", "source": "\ntrace = go.Scatter(\n    x = df_ce_demand['PRICE'],\n    y = df_ce_demand['Q_HAT'],\n    mode = 'lines'\n)\n\n\n\nlayout = go.Layout(\n    title='Demand Curve',\n    xaxis=dict(\n        title='Price',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    ),\n    yaxis=dict(\n        title='Quantity',\n        titlefont=dict(\n            family='Courier New, monospace',\n            size=18,\n            color='#7f7f7f'\n        )\n    )\n)\n    \ndata=[trace]  \nfig = go.Figure(data=data, layout=layout)\n\n#plot_url = py.plot(fig, filename='styling-names')\nplotly.offline.iplot(fig, filename='shapes-lines')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Wait a minute! That's not linear! Nope, it isn't. This a variation of the demand curves we messed around with in part 1 of our exercise. This is what we call a constant elasticity demand curve. The elasticity is the same, at all points. \n\nRemember in our earlier discussion that elasticity was different at each price point of the demand curve. With this demand curve, elasticity is the same at all the price points. It still shows the relationship between price and quantity. \n\nFor example, at a price of 6 dollars, our firm can sell about 75,000 units at each store.\n\n\nIf you wanted a linear demand curve, you can definitely get there. You would regress price on quantity (instead of ln of price on ln of quantity). Of course, if you built your model this way, the coefficient wouldn't be an elasticity."}, {"metadata": {}, "cell_type": "markdown", "source": "# Estimate Price Elasticity of Revenue"}, {"metadata": {}, "cell_type": "markdown", "source": "Estimating the price elasticity of revenue follows a similar process. The difference is we will use the natural log of revenue as a dependent variable instead of the natural log of quantity."}, {"metadata": {}, "cell_type": "code", "source": "independentx = df_retail[['LN_PRICE','LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n                          'LN_PERCENT_SPEAKING_SPANISH']]\nindependent = sm.add_constant(independentx, prepend=False)\ndependent=df_retail['LN_REVENUE']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "mod = sm.OLS(dependent, independent)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "res = mod.fit()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(res.summary())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The regression results suggest that price elasticity of revenue is\u00a0.358. This means that a 1% increase in price will lead to a\u00a0.358% increase in revenue. In other words, this firm would make more revenue if it increased prices.\n\nThere are few important caveats that I should probably mention.\n\nOne, this is a point estimate. That is a fancy way of saying that you shouldn't get too crazy. If you increase prices by 1% you probably will increase revenue by\u00a0.35%. However, if you increase prices by 100%, you probably would not realize a 35% increase in revenue. Baked into the model is an established historical relationship between your customers and your prices. If you do something that is way outside of the historical norm, don't expect the model to be predictive.\n\nTwo, it is important to understand this elasticity is an average across all stores. The price elasticity of revenue is\u00a0.358, on average. There are 291 stores in the data set. Some probably have an elasticity greater than\u00a0.35. Others probably have an elasticity that is less than\u00a0.35. In other words, if you increase prices by 3% across the board, on average, you will realize a 1.05% increase in revenue. This is an average. Some stores will realize more than 1.05%. Others will realize less than 1.05%. A 3% increase in prices may even cause some stores to lose revenue.\n\nWhat if you could tailor the price increase for each store? \n\nThat is, increase prices by an average of 3%, but give some stores a higher bump in prices than others. You could even decrease prices in some stores if it makes sense. Tailoring the price increase to each store based on their specific market, will lead to an even greater increase in revenue. For example, you can raise prices by 3% on average and get an increase in revenue greater than 1.05%.\n\n\nThere are many ways to accomplish this goal. In the third part of this exercise, we will examine a relatively simple and straight forward way to make a market-based pricing decision for each of our 291 stores.\n"}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}